{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fabba25e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T11:45:57.704068Z",
     "start_time": "2022-11-14T11:45:57.699357Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "150bc8f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T11:45:59.799199Z",
     "start_time": "2022-11-14T11:45:57.707293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') \n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb70d4dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T11:45:59.811401Z",
     "start_time": "2022-11-14T11:45:59.805356Z"
    }
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'EPOCHS': 20,\n",
    "    'LEARNING_RATE':1e-3,\n",
    "    'BATCH_SIZE':512,\n",
    "    'SEED':101\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4087060",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T11:45:59.826103Z",
     "start_time": "2022-11-14T11:45:59.814765Z"
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG['SEED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6593e0d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T11:45:59.925692Z",
     "start_time": "2022-11-14T11:45:59.828839Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adef72c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89b2782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ece7c33f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T11:45:59.932677Z",
     "start_time": "2022-11-14T11:45:59.927931Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_features = ['COMPONENT_ARBITRARY', 'YEAR']\n",
    "# Inference(실제 진단 환경)에 사용하는 컬럼\n",
    "test_stage_features = ['COMPONENT_ARBITRARY', 'ANONYMOUS_1', 'YEAR' , 'ANONYMOUS_2', 'AG', 'CO', 'CR', 'CU', 'FE', 'H2O', 'MN', 'MO', 'NI', 'PQINDEX', 'TI', 'V', 'V40', 'ZN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cfa7cb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T11:45:59.951690Z",
     "start_time": "2022-11-14T11:45:59.934503Z"
    }
   },
   "outputs": [],
   "source": [
    "train = train.fillna(0)\n",
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a95d2928",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T11:45:59.979505Z",
     "start_time": "2022-11-14T11:45:59.953740Z"
    }
   },
   "outputs": [],
   "source": [
    "all_X = train.drop(['ID', 'Y_LABEL'], axis = 1)\n",
    "all_y = train['Y_LABEL']\n",
    "\n",
    "test = test.drop(['ID'], axis = 1)\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(all_X, all_y, test_size=0.2, random_state=CONFIG['SEED'], stratify=all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99a5e5e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T11:45:59.997213Z",
     "start_time": "2022-11-14T11:45:59.984268Z"
    }
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "for col in categorical_features:    \n",
    "    train_X[col] = le.fit_transform(train_X[col])\n",
    "    val_X[col] = le.transform(val_X[col])\n",
    "    if col in test.columns:\n",
    "        test[col] = le.transform(test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaf8f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "853cb4eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T11:46:00.036457Z",
     "start_time": "2022-11-14T11:45:59.998978Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_features = train_X.head(1).drop(categorical_features,axis=1).columns\n",
    "\n",
    "# ---- TRAIN DATA\n",
    "trainX_scaled = pd.DataFrame(scaler.fit_transform(train_X.drop(categorical_features,axis=1)),columns=scaled_features)\n",
    "trainX_scaled[categorical_features] = train_X[categorical_features].reset_index(drop=True)\n",
    "trainX_scaled = trainX_scaled[train_X.columns]\n",
    "\n",
    "# ---- VAL DATA\n",
    "valX_scaled = pd.DataFrame(scaler.transform(val_X.drop(categorical_features,axis=1)),columns=scaled_features)\n",
    "valX_scaled[categorical_features] = val_X[categorical_features].reset_index(drop=True)\n",
    "valX_scaled = valX_scaled[val_X.columns]\n",
    "\n",
    "# ---- LABEL TRAIN and TEST\n",
    "train_y = pd.DataFrame(train_y).reset_index(drop=True)\n",
    "val_y = pd.DataFrame(val_y).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "905c9a9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T11:46:00.048660Z",
     "start_time": "2022-11-14T11:46:00.038959Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_X, data_y, distillation=False):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.data_X = data_X\n",
    "        self.data_y = data_y\n",
    "        self.distillation = distillation\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.distillation:\n",
    "            #------------------------------------\n",
    "            # When Learing with distillation\n",
    "            #------------------------------------\n",
    "            teacher_X = torch.Tensor(self.data_X.iloc[index])\n",
    "            student_X = torch.Tensor(self.data_X[test_stage_features].iloc[index])\n",
    "            y = self.data_y.values[index]\n",
    "            return  teacher_X.to(device), \\\n",
    "                    student_X.to(device), \\\n",
    "                    torch.Tensor(y).to(device)\n",
    "        else:\n",
    "            #------------------------------------\n",
    "            # When Learing with Normal Data\n",
    "            #------------------------------------\n",
    "            if self.data_y is None:\n",
    "                test_X = torch.Tensor(self.data_X.iloc[index])\n",
    "                return test_X.to(device)\n",
    "            else:\n",
    "                teacher_X = torch.Tensor(self.data_X.iloc[index])\n",
    "                y = self.data_y.values[index]\n",
    "                return  teacher_X.to(device), \\\n",
    "                        torch.Tensor(y).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e6b0534",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T11:46:00.056803Z",
     "start_time": "2022-11-14T11:46:00.050202Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(trainX_scaled, train_y, False)\n",
    "val_dataset = CustomDataset(valX_scaled, val_y, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6f12396",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T11:46:00.063850Z",
     "start_time": "2022-11-14T11:46:00.059408Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = CONFIG['BATCH_SIZE'], shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size = CONFIG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbde09ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T11:46:00.075310Z",
     "start_time": "2022-11-14T11:46:00.066652Z"
    }
   },
   "outputs": [],
   "source": [
    "class Teacher(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Teacher, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=52, out_features=256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=256, out_features=1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=1024, out_features=256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=256, out_features=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.classifier(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d838b56b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T11:46:00.093084Z",
     "start_time": "2022-11-14T11:46:00.077385Z"
    }
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# METRICS\n",
    "#--------------------------------------------------------------------------------\n",
    "def competition_metric(true, pred):\n",
    "    return f1_score(true, pred, average=\"macro\")\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# TRAINING\n",
    "#--------------------------------------------------------------------------------\n",
    "def teacher_train(model, optimizer, train_loader, val_loader, scheduler):\n",
    "    model.to(device)\n",
    "\n",
    "    best_score = 0\n",
    "    best_model = None\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "\n",
    "    for epoch in range(CONFIG[\"EPOCHS\"]):\n",
    "        train_loss = []\n",
    "        \n",
    "        # ACTIVATE TRAINING MODE\n",
    "        # --- normalisation layers1 use per-batch statistics\n",
    "        # --- activates Dropout layers2\n",
    "        model.train()\n",
    "    \n",
    "        for i,(X, y) in tqdm(enumerate(train_loader)):\n",
    "            \n",
    "            #X = X.float().to(device)\n",
    "            #y = y.float().to(device)\n",
    "            \n",
    "            # ZERO GRADIENT\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # FORWARD\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, y)\n",
    "            \n",
    "            # BACKWARD\n",
    "            loss.backward()\n",
    "            \n",
    "            # UPDATE\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        val_loss, val_score = validation_teacher(model, val_loader, criterion)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{np.mean(train_loss) :.5f}] Val Loss : [{np.mean(val_loss) :.5f}] Val F1 Score : [{val_score:.5f}]')\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_score)\n",
    "            \n",
    "        if best_score < val_score:\n",
    "            best_model = model\n",
    "            best_score = val_score\n",
    "        \n",
    "    return best_model \n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# VALIDATION\n",
    "#--------------------------------------------------------------------------------\n",
    "def validation_teacher(model, val_loader, criterion):\n",
    "    # ACTIVATE EVALUATION MODE\n",
    "    # --- normalisation layers use running statistics\n",
    "    # --- de-activates Dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = []\n",
    "    pred_labels = []\n",
    "    true_labels = []\n",
    "    threshold = 0.5\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i,(X, y) in tqdm(enumerate(val_loader)):\n",
    "#             X = X.float().to(device)\n",
    "#             y = y.float().to(device)\n",
    "            \n",
    "            model_pred = model(X)\n",
    "            \n",
    "            loss = criterion(model_pred,y)\n",
    "            val_loss.append(loss.item())      \n",
    "            \n",
    "            model_pred = model_pred.squeeze(1)#.to('cpu')  \n",
    "            pred_labels += model_pred.tolist()\n",
    "            true_labels += y.tolist()\n",
    "        \n",
    "        pred_labels = np.where(np.array(pred_labels) > threshold, 1, 0)\n",
    "        val_f1 = competition_metric(true_labels, pred_labels)\n",
    "    return val_loss, val_f1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf6cfb59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T11:46:07.403001Z",
     "start_time": "2022-11-14T11:46:00.095525Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.0000,  0.7868, 11.0000, -0.3945, -0.3407, -0.1505, -0.1192, -0.5691,\n",
       "         -0.3078, -0.0426, -0.1569, -0.0651, -0.0850, -0.1077, -0.2462, -0.3642,\n",
       "         -0.5897, -0.3646, -0.5866, -0.6048, -0.6063, -0.3066, -0.1057, -0.0399,\n",
       "         -0.2196, -0.1080, -0.2771, -0.2369, -0.3839, -0.2065, -0.1846, -1.1619,\n",
       "         -0.1500, -0.2537, -0.4782, -0.1723, -0.1712, -0.2364, -0.3834, -0.1004,\n",
       "         -0.0764, -0.0997,  0.2821,  0.1438,  0.0344,  0.0875,  0.7378,  1.4414,\n",
       "         -0.1103, -0.5949, -0.8780, -0.5356], device='cuda:0'),\n",
       " tensor([0.], device='cuda:0'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adec7c17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T11:47:33.297830Z",
     "start_time": "2022-11-14T11:46:07.406420Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:03,  6.54it/s]\n",
      "6it [00:00,  7.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], Train Loss : [0.30647] Val Loss : [0.22883] Val F1 Score : [0.71587]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:03,  6.68it/s]\n",
      "6it [00:00,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.20408] Val Loss : [0.19778] Val F1 Score : [0.77058]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:03,  6.47it/s]\n",
      "6it [00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.20864] Val Loss : [0.19336] Val F1 Score : [0.77245]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:03,  6.49it/s]\n",
      "6it [00:00,  7.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.18006] Val Loss : [0.18930] Val F1 Score : [0.77830]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:03,  6.61it/s]\n",
      "6it [00:00,  7.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.16672] Val Loss : [0.18968] Val F1 Score : [0.79069]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:03,  6.62it/s]\n",
      "6it [00:00,  7.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [0.17931] Val Loss : [0.17594] Val F1 Score : [0.79831]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:03,  6.62it/s]\n",
      "6it [00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6], Train Loss : [0.18818] Val Loss : [0.17980] Val F1 Score : [0.77899]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:03,  6.66it/s]\n",
      "6it [00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7], Train Loss : [0.16228] Val Loss : [0.18103] Val F1 Score : [0.78525]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:03,  6.47it/s]\n",
      "6it [00:00,  7.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8], Train Loss : [0.14524] Val Loss : [0.17049] Val F1 Score : [0.78789]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:03,  6.59it/s]\n",
      "6it [00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9], Train Loss : [0.14809] Val Loss : [0.17003] Val F1 Score : [0.80042]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:03,  6.82it/s]\n",
      "6it [00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10], Train Loss : [0.14572] Val Loss : [0.17567] Val F1 Score : [0.79041]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:03,  6.86it/s]\n",
      "6it [00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11], Train Loss : [0.13512] Val Loss : [0.17004] Val F1 Score : [0.79348]\n",
      "Epoch    12: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:03,  6.86it/s]\n",
      "6it [00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12], Train Loss : [0.12787] Val Loss : [0.16791] Val F1 Score : [0.79941]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:03,  6.90it/s]\n",
      "6it [00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13], Train Loss : [0.13378] Val Loss : [0.16873] Val F1 Score : [0.80813]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:03,  6.85it/s]\n",
      "6it [00:00,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14], Train Loss : [0.13808] Val Loss : [0.17148] Val F1 Score : [0.79556]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:03,  6.56it/s]\n",
      "6it [00:00,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15], Train Loss : [0.13234] Val Loss : [0.16895] Val F1 Score : [0.79927]\n",
      "Epoch    16: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:03,  6.78it/s]\n",
      "6it [00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16], Train Loss : [0.12936] Val Loss : [0.16955] Val F1 Score : [0.80909]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:03,  6.78it/s]\n",
      "6it [00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17], Train Loss : [0.12617] Val Loss : [0.17084] Val F1 Score : [0.79631]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:03,  6.82it/s]\n",
      "6it [00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18], Train Loss : [0.12315] Val Loss : [0.17085] Val F1 Score : [0.81099]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:03,  6.78it/s]\n",
      "6it [00:00,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19], Train Loss : [0.11937] Val Loss : [0.17173] Val F1 Score : [0.81005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = Teacher()\n",
    "# model.eval()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['LEARNING_RATE'])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=1, threshold_mode='abs',min_lr=1e-8, verbose=True)\n",
    "\n",
    "teacher_model = teacher_train(model, optimizer, train_loader, val_loader, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d97a84d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8604c9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95896270",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T11:47:33.308732Z",
     "start_time": "2022-11-14T11:47:33.300179Z"
    }
   },
   "outputs": [],
   "source": [
    "class Student(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Student, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=18, out_features=128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=128, out_features=512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=512, out_features=128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=128, out_features=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.classifier(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6db0ea9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T11:47:33.318987Z",
     "start_time": "2022-11-14T11:47:33.310567Z"
    }
   },
   "outputs": [],
   "source": [
    "def distillation_loss(student_logits, labels, teacher_logits, alpha):\n",
    "    distillation_loss = nn.BCELoss()(student_logits, teacher_logits)\n",
    "    student_loss = nn.BCELoss()(student_logits, labels.reshape(-1, 1))\n",
    "    return alpha * student_loss + (1-alpha) * distillation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561c57f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
